{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siam_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TdUQnlDWeMD",
        "colab_type": "code",
        "outputId": "a5e2f293-1498-458e-941e-94ddae0bd2d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo_cWIBddq0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "#import the necessary libraries\n",
        "\"\"\"\n",
        "We will be using Tensorflow of version 1.x\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import datetime\n",
        "from zipfile import ZipFile\n",
        "from os.path import expanduser, exists\n",
        "import time\n",
        "import json\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import np_utils\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras import activations\n",
        "from keras import losses\n",
        "from keras import metrics\n",
        "from keras.models import load_model\n",
        "from keras import layers\n",
        "from keras import activations\n",
        "from keras import losses\n",
        "from keras import optimizers\n",
        "from keras import metrics\n",
        "from keras import Input\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import merge\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import concatenate\n",
        "from keras.layers import Dense\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Lambda,TimeDistributed,Dot,dot\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe7QzJK7jRFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Read the data:\n",
        "\"\"\"\n",
        "i/p: Path of the folder where the dataset is placed\n",
        "o/p: dataframe\n",
        "\"\"\"\n",
        "path = \"/content/drive/My Drive/Project/Data/\" #give the path where questions.csv is saved\n",
        "df = pd.read_csv(path + \"questions.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6Kx_SP7jo0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create an extra copy of the dataframe\n",
        "train_df = df.copy()\n",
        "train_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi1aGHqczL0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df_copy = train_df.copy()\n",
        "train_df_copy.describe()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLjwJ5uQz9km",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#final dataframe copy for training\n",
        "train_DF = train_df.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH9RShFo3SD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Store columns as lists to make it model ready\n",
        "\"\"\"\n",
        "i/p: Columns of DataFrame\n",
        "o/p: list of values for each column\n",
        "\"\"\"\n",
        "q1_l = train_DF['question1'].tolist()\n",
        "q1_l = [str(q) for q in q1_l]\n",
        "q2_l = train_DF['question2'].tolist()\n",
        "q2_l = [str(q) for q in q2_l]\n",
        "label_list = train_DF['is_duplicate'].tolist()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-DX3WBa3ZJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenize on the entire data,get the word sequences,word_indx.This will be the input to the model\n",
        "\"\"\"\n",
        "i/p: q1_list+q2_list\n",
        "o/p:dict_word_indx , key = word and value = word_index\n",
        "\"\"\" \n",
        "total_ques = q1_l + q2_l\n",
        "token = Tokenizer(num_words=100000)\n",
        "token.fit_on_texts(total_ques)\n",
        "\n",
        "q1_seq = token.texts_to_sequences(q1_l)\n",
        "q2_seq = token.texts_to_sequences(q2_l)\n",
        "word_indx = token.word_index\n",
        "dict_word_indx = word_indx\n",
        "with open(path + 'word_indx.json', 'w') as dfile:\n",
        "    json.dump(dict_word_indx, dfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8tb2oKD6RjN",
        "colab_type": "code",
        "outputId": "e0039b6c-f628-4abc-cea4-0a3b40b8f431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#download GLOVE embeddings:\n",
        "# Citations: Stackoverflow.com,'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
        "\"\"\"\n",
        "i/p: GloVe\n",
        "o/p: embeddings_index dict\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "GLOVE_URL = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
        "\n",
        "if not exists(expanduser('~/.keras/datasets/glove.840B.300d.zip')):\n",
        "    zipfile = ZipFile(get_file('glove.840B.300d.zip', GLOVE_URL))\n",
        "    zipfile.extract('glove.840B.300d.txt', path=expanduser('~/.keras/datasets/'))\n",
        "    \n",
        "print(\"Processing\", 'glove.840B.300d.txt')\n",
        "\n",
        "embed_indx = {}\n",
        "\n",
        "with open(expanduser('~/.keras/datasets/glove.840B.300d.txt'), encoding='utf-8') as filee:\n",
        "    for l in filee:\n",
        "        val = l.split(' ')\n",
        "        w = val[0]\n",
        "        embed = np.asarray(val[1:], dtype='float32')\n",
        "        embed_indx[w] = embed\n",
        "\n",
        "print('Word embeddings: %d' % len(embed_indx))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing glove.840B.300d.txt\n",
            "Word embeddings: 2196016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRPNO8O7_1S2",
        "colab_type": "code",
        "outputId": "e06ca4ea-f44d-41b7-ab0e-a502736ffb07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Embedding matrix\n",
        "\"\"\"\n",
        "I/p: word sequences\n",
        "o/p: embedded vectors of words\n",
        "\"\"\"\n",
        "dim_embed = 300\n",
        "word_max = 100000\n",
        "nwords = min(word_max, len(word_indx))\n",
        "w_embed_matr = np.zeros((nwords + 1, dim_embed))\n",
        "for w, i in word_indx.items():\n",
        "    if i > word_max:\n",
        "        continue\n",
        "    embedd_vector = embed_indx.get(w)\n",
        "    if embedd_vector is not None:\n",
        "        w_embed_matr[i] = embedd_vector\n",
        "\n",
        "#print('Null word embeddings: %d' % np.sum(np.sum(w_embed_matr, axis=1) == 0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Null word embeddings: 29273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFGYEcZLADCv",
        "colab_type": "code",
        "outputId": "1fd3a14a-5144-4b38-f53e-4cf6f9e73ca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "\n",
        "#q1_seq[0:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 3, 1, 1228, 57, 1228, 2582, 7, 575, 8, 763, 383, 8, 35],\n",
              " [2, 3, 1, 558, 10, 14302, 13600, 5, 21317, 4565],\n",
              " [4, 13, 5, 217, 1, 440, 10, 17, 361, 1827, 200, 146, 6, 2773],\n",
              " [16, 72, 5, 2774, 307, 2757, 4, 13, 5, 648, 19],\n",
              " [23, 49, 7130, 8, 230, 35502, 1893, 2046, 10573, 12, 1928, 10923, 6461]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqHqP06AAdDy",
        "colab_type": "code",
        "outputId": "271c5a87-5e26-4042-9463-b1ed5c0e9630",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Padding the sequences with zeros, length of each sequence = max_length of sequence\n",
        "\"\"\"\n",
        "i/p:sequence\n",
        "o/p:padded sequence,X- features,y-label\n",
        "\"\"\"\n",
        "max_len_seq = 0\n",
        "for q in q1_seq:\n",
        "    if(len(q) > max_len_seq):\n",
        "        max_len_seq = len(q)\n",
        "max_len_seq\n",
        "max_len_seq= 130\n",
        "q1_d = pad_sequences(q1_seq, maxlen=max_len_seq)\n",
        "q2_d = pad_sequences(q2_word_seq, maxlen=max_len_seq)\n",
        "label = np.array(label_list, dtype=int)\n",
        "X = np.stack((q1_d, q2_d), axis=1)\n",
        "y = label\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "127"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2cva5udBGEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model Preparation\n",
        "#Split the data as train,valid and test in the ratio 7:1:2\n",
        "train_ratio = 0.70\n",
        "validation_ratio = 0.10\n",
        "test_ratio = 0.20\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-train_ratio)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio))\n",
        "Q1_train = X_train[:,0]\n",
        "Q2_train = X_train[:,1]\n",
        "Q1_test = X_test[:,0]\n",
        "Q2_test = X_test[:,1]\n",
        "Q1_val = X_val[:,0]\n",
        "Q2_val = X_val[:,1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEsXQWEUBWyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#construct the siamese neural network using the manhattan distance:\n",
        "def exp_manhattan_distance(l1, l2):\n",
        "    '''Manhattan distance as similarity measure of inputs'''\n",
        "    return K.exp(-K.sum(K.abs(l1-l2), axis=1, keepdims=True))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moOOoygfQoWu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1631b30b-b4b5-42a9-919f-d6d7c2f480e7"
      },
      "source": [
        "#Building the model:\n",
        "\"\"\"\n",
        "i/p: input sequence of question1 and question 2\n",
        "o/p: 2LSTMs combined by manhattan distance as the output,a single value :0 or 1\n",
        "\"\"\"\n",
        "q1 = Input(shape=(max_len_seq,))\n",
        "q2 = Input(shape=(max_len_seq,))\n",
        "ques1 = Embedding(nwords + 1, \n",
        "                 dim_embed, \n",
        "                 weights=[w_embed_matr], \n",
        "                 input_length=max_len_seq, \n",
        "                 trainable=False)(q1)\n",
        "\n",
        "\n",
        "ques2 = Embedding(nwords + 1, \n",
        "                 dim_embed, \n",
        "                 weights=[w_embed_matr], \n",
        "                 input_length=max_len_seq, \n",
        "                 trainable=False)(q2)\n",
        "\n",
        "\n",
        "n_hidden = 100\n",
        "s_lstm = LSTM(n_hidden)\n",
        "left = s_lstm(ques1)\n",
        "right = s_lstm(ques2)\n",
        "distance = Lambda(function=lambda x: exp_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left, right])\n",
        "\n",
        "#final model\n",
        "model = Model(input = [q1,q2], output = [distance])\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDgn0qUIKdE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save the model\n",
        "model_j = model.to_json()\n",
        "with open(path + 'model.json', 'w') as json_f:\n",
        "    json_f.write(model_j)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0HfxEzhDAgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train the model with inputs as Q1_train,Q2_train\n",
        "print(\"Start time\", datetime.datetime.now())\n",
        "st_time = time.time()\n",
        "his_ = model.fit([Q1_train, Q2_train],\n",
        "                    y_train,\n",
        "                    epochs=25,\n",
        "                    validation_data=([Q1_val, Q2_val], y_val),\n",
        "                    verbose=1,\n",
        "                    batch_size=64)\n",
        "end_time = time.time()\n",
        "print(\"End Time\", datetime.datetime.now())\n",
        "print(\"Total Time: %f\" % ((end_time - st_time) / 60.))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYhI5jrVP6n-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fit the model with validation,test to get the necessary evaluation metrics\n",
        "values = model.evaluate(x=[Q1_val, Q2_val], y=y_val)\n",
        "print(\"Loss val: \" + str(values[0]))\n",
        "print(\"Accuracy val: \" + str(values[1]))\n",
        "\n",
        "values_t = model.evaluate(x=[Q1_test, Q2_test], y=y_test)\n",
        "print(\"Loss test: \" + str(values_t[0]))\n",
        "print(\"Accuracy test: \" + str(values_t[1]))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXD599JaSxDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model evaluation\n",
        "\"\"\"\n",
        "i/p:validation, test\n",
        "o/p: results_val.txt,results_test.txt\n",
        "\"\"\"\n",
        "\n",
        "yhat_probs_val = model.predict([Q1_val, Q2_val], verbose=0)\n",
        "yhat_probs_test = model.predict([Q1_test, Q2_test], verbose=0)\n",
        "y_pred = np.ravel(yhat_probs_val).tolist()\n",
        "df_pred = pd.DataFrame()\n",
        "df_pred['pred_classes']= y_pred\n",
        "y_val = np.ravel(y_val).tolist()\n",
        "df_pred['True Y_val'] = y_val\n",
        "excel_ = df_pred.to_excel(path + \"Pred_val_2.xlsx\",index = None, header = True)\n",
        "\n",
        "y_pred_t = np.ravel(yhat_probs_test).tolist()\n",
        "df_pred_t = pd.DataFrame()\n",
        "df_pred_t['pred_classes']= y_pred_t\n",
        "y_test = np.ravel(y_test).tolist()\n",
        "df_pred_t['True Y_test'] = y_test\n",
        "excel_t = df_pred_t.to_excel(path + \"Pred_test.xlsx\",index = None, header = True)\n",
        "\n",
        "\n",
        "y_pr_val = (df_pred['pred_classes']).tolist()\n",
        "y_val_tr = (df_pred['True Y_val']).tolist()\n",
        "y_pr_class_val = [(lambda i: 1 if i >= 0.5 else 0)(i) for i in y_pr_val]\n",
        "\n",
        "y_pr_test = (df_pred_t['pred_classes']).tolist()\n",
        "y_test_tr = (df_pred_t['True Y_test']).tolist()\n",
        "y_pr_class_test = [(lambda i: 1 if i >= 0.5 else 0)(i) for i in y_pr_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT6EBuPCXnDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "with open(path + \"Result_SiMaLSTM_val.txt\",\"a\",encoding = \"UTF-8\") as f:\n",
        "  accuracy = accuracy_score(y_val_tr, y_pr_class_val)\n",
        "  print('Accuracy: %f' % accuracy)\n",
        "  f.write(\"Accuracy:\" + str(accuracy) + \"\\n\")\n",
        "  precision = precision_score(y_val_tr, y_pr_class_val)\n",
        "  print('Precision: %f' % precision)\n",
        "  f.write(\"precision:\" + str(precision) + \"\\n\")\n",
        "  # recall: tp / (tp + fn)\n",
        "  recall = recall_score(y_val_tr, y_pr_class_val)\n",
        "  print('Recall: %f' % recall)\n",
        "  f.write(\"recall:\" + str(recall) + \"\\n\")\n",
        "  # f1: 2 tp / (2 tp + fp + fn)\n",
        "  f1 = f1_score(y_val_tr, y_pr_class_val)\n",
        "  print('F1 score: %f' % f1)\n",
        "  f.write(\"f1 score:\" + str(f1) + \"\\n\")\n",
        "  auc = roc_auc_score(y_val_tr, y_pr_class_val)\n",
        "  print('ROC score: %f' % auc)\n",
        "  f.write(\"ROC score:\" + str(auc) + \"\\n\")\n",
        "  # confusion matrix\n",
        "  matrix = confusion_matrix(y_val_tr, y_pr_class_val)\n",
        "  print(matrix)\n",
        "\n",
        "\n",
        "\n",
        "with open(path + \"Result_SiMaLSTM_test.txt\",\"a\",encoding = \"UTF-8\") as f:\n",
        "  accuracy = accuracy_score(y_test_tr, y_pr_class_test)\n",
        "  print('Accuracy: %f' % accuracy)\n",
        "  f.write(\"Accuracy:\" + str(accuracy) + \"\\n\")\n",
        "  precision = precision_score(y_val_tr, y_pr_class_val)\n",
        "  print('Precision: %f' % precision)\n",
        "  f.write(\"precision:\" + str(precision) + \"\\n\")\n",
        "  # recall: tp / (tp + fn)\n",
        "  recall = recall_score(y_val_tr, y_pr_class_val)\n",
        "  print('Recall: %f' % recall)\n",
        "  f.write(\"recall:\" + str(recall) + \"\\n\")\n",
        "  # f1: 2 tp / (2 tp + fp + fn)\n",
        "  f1 = f1_score(y_val_tr, y_pr_class_val)\n",
        "  print('F1 score: %f' % f1)\n",
        "  f.write(\"f1 score:\" + str(f1) + \"\\n\")\n",
        "  auc = roc_auc_score(y_val_tr, y_pr_class_val)\n",
        "  print('ROC score: %f' % auc)\n",
        "  f.write(\"ROC score:\" + str(auc) + \"\\n\")\n",
        "  # confusion matrix\n",
        "  matrix = confusion_matrix(y_val_tr, y_pr_class_val)\n",
        "  print(matrix)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}